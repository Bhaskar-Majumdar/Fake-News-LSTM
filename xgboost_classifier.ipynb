{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "xgboost_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bhaskar-Majumdar/Fake-News-LSTM/blob/main/xgboost_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e324JoRbZiX"
      },
      "source": [
        "<h4>Fake News Classification. (Task3a) </h4>\n",
        "    <p>Fake news classifier model with 4 output classess:</p>\n",
        "    <ol>\n",
        "    <li>FALSE</li>\n",
        "    <li>TRUE</li>\n",
        "    <li>partially false</li>\n",
        "    <li>other</li>\n",
        "    </ol>\n",
        "<h5>This machine learning model is a XGBclassifier model that usually works very well with text data</h5>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRP8O3qabZih"
      },
      "source": [
        "Importing processed final training data and test data as pandas dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYF1m067bZih"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiWZxxvCbZii"
      },
      "source": [
        "train = pd.read_csv('final_stemmed_data.csv')\n",
        "test = pd.read_csv('Task3a_testing.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jXBnEADbZii",
        "outputId": "b845fb7d-73a3-4525-de80-00534917e004"
      },
      "source": [
        "# Checking For null values\n",
        "print(len(train))\n",
        "print(train.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "59494\n",
            "text          633\n",
            "our rating      0\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63GJrb-ibZij"
      },
      "source": [
        "# dropping all the null values\n",
        "train=train.dropna(axis=0)\n",
        "train.reset_index(inplace=True)\n",
        "train = train.drop('index', axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMI0PedKbZij",
        "outputId": "4d70d4fb-9416-4eb7-8dae-20d99f015816"
      },
      "source": [
        "# After droping null values\n",
        "print(len(train))\n",
        "print(train.isnull().sum())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "58861\n",
            "text          0\n",
            "our rating    0\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5qXqG9KbZik"
      },
      "source": [
        "<h4>Text processing functions.</h4>\n",
        "<h6> This functions will convert our Task3a_testing data to our Classifier feedable vector data so that we can predict of from the test data. </h6>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LINHamp_bZik"
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SatuMeRtbZik",
        "outputId": "b45d467c-fe8d-456a-b579-1e6e20cdcc79"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HawjW19AbZil"
      },
      "source": [
        "# text corpus to count_vectorized data converter function\n",
        "def count_vectorization(corpus, max_features = 5000, ngram_range = (1,3)):\n",
        "    \n",
        "    cv = CountVectorizer(max_features=max_features,ngram_range=ngram_range)\n",
        "    X = cv.fit_transform(corpus).toarray()\n",
        "    \n",
        "    return (X,cv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7geHLzucbZil"
      },
      "source": [
        "# stemming and stop words removing function\n",
        "def text_process(df):\n",
        "    ps = PorterStemmer()\n",
        "    corpus = []\n",
        "    for i in range(0, len(df)):\n",
        "        print(i)\n",
        "        review = re.sub('[^a-zA-Z]', ' ', str(df['input'][i]))\n",
        "        review = review.lower()\n",
        "        review = review.split()\n",
        "\n",
        "        review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
        "        review = ' '.join(review)\n",
        "        corpus.append(review)\n",
        "        \n",
        "    \n",
        "    return corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrjDG2p_bZim"
      },
      "source": [
        "# splitting training data into data and label as  X_data and y_data \n",
        "X_data=train.drop(['our rating'],axis=1)\n",
        "y_data=train['our rating']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TdjG2UFbZim",
        "outputId": "73dcbb04-9be7-4e1d-f0a5-be16305bf152"
      },
      "source": [
        "X_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>last week rep louie gohmert told chri salcedo ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>washington reuter head conserv republican fact...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>donald trump wish american happi new year leav...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>distract drive caus death canada impair drive ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>whatev drama play republican meet cleveland ne...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text\n",
              "0  last week rep louie gohmert told chri salcedo ...\n",
              "1  washington reuter head conserv republican fact...\n",
              "2  donald trump wish american happi new year leav...\n",
              "3  distract drive caus death canada impair drive ...\n",
              "4  whatev drama play republican meet cleveland ne..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrijLo0EbZim",
        "outputId": "6cc48e4c-c731-4c69-b055-10483d3d884e"
      },
      "source": [
        "y_data.unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['FALSE', 'TRUE', 'partially false', 'other'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeqQTC_mbZin"
      },
      "source": [
        "X_data, cv = count_vectorization(X_data['text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daMLkEZ-bZin",
        "outputId": "1810d6e6-e5d5-43b9-9e73-d1f5ecac56ef"
      },
      "source": [
        "print(X_data.shape)\n",
        "print(y_data.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(58861, 5000)\n",
            "(58861,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ovL2vuubZio"
      },
      "source": [
        "<h6>Splitting total trainig data into trainig and validation data with 80:20 ratio.</h6>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxU-pdVQbZio"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDb3kfnGbZio"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJz3i0ehbZip"
      },
      "source": [
        "<h3>Defining our ML classifier</h3>\n",
        "<h3>This classifier model is XGBClassifer from xgboost library</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5U1zyzcbZip"
      },
      "source": [
        "from xgboost import XGBClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LDUZ0MqbZip",
        "outputId": "3afa8549-7aa8-43fa-b5a2-4502f43308f8"
      },
      "source": [
        "classifier = XGBClassifier()\n",
        "classifier.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\user\\anaconda3\\envs\\lab_env\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
            "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[15:04:53] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
              "              importance_type='gain', interaction_constraints='',\n",
              "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
              "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
              "              n_estimators=100, n_jobs=6, num_parallel_tree=1,\n",
              "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
              "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
              "              tree_method='exact', validate_parameters=1, verbosity=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnYpjK1BbZiq"
      },
      "source": [
        "Y_pred = classifier.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaypYJPWbZiq"
      },
      "source": [
        "Viewing classification report:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojf4MZAPbZiq"
      },
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRgR1SgObZiq",
        "outputId": "df18bd2e-78ee-409c-b24a-09dcb5eb9509"
      },
      "source": [
        "print(classification_report(y_test, Y_pred)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                 precision    recall  f1-score   support\n",
            "\n",
            "          FALSE       0.89      0.98      0.94      6596\n",
            "           TRUE       0.99      0.99      0.99      4326\n",
            "          other       0.49      0.11      0.19       819\n",
            "partially false       0.57      0.12      0.21        32\n",
            "\n",
            "       accuracy                           0.92     11773\n",
            "      macro avg       0.74      0.55      0.58     11773\n",
            "   weighted avg       0.90      0.92      0.90     11773\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw1JQEC1bZir",
        "outputId": "f72f13a0-1d35-48a8-8163-78cf6542a2c5"
      },
      "source": [
        "print('Model Accuracy: ',accuracy_score(y_test,Y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Accuracy:  0.9227894334494181\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEa3dJObbZir"
      },
      "source": [
        "<h3>Prediction of the test set</h3>\n",
        "<h4>And exporting the result as a tsv file</h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DesMkWIwbZir",
        "outputId": "e85cdee0-726d-411f-a1ab-fd687c6f8ed7"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>public_id</th>\n",
              "      <th>text</th>\n",
              "      <th>title</th>\n",
              "      <th>our rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>81a67c96</td>\n",
              "      <td>Former state House Majority Leader Adam Hasner...</td>\n",
              "      <td>- The Washington Post</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6e5ec6fb</td>\n",
              "      <td>Editor’s note: Subsequent to this article, VTD...</td>\n",
              "      <td>Rubio Comments on Iran Nuclear Deal</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>d9cd4895</td>\n",
              "      <td>The hypocritical Lib Dems want to ignore the r...</td>\n",
              "      <td>Climate Alarmists Caught Manipulating Temperat...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4a1a9b9f</td>\n",
              "      <td>Urgent action to boost the number of children ...</td>\n",
              "      <td>Who are the arsonists setting rural fires in W...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6d16fa40</td>\n",
              "      <td>By Ken Allen AFSCME Council 75 represents the ...</td>\n",
              "      <td>Diabetes prescriptions now cost NHS £1bn, figu...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  public_id                                               text  \\\n",
              "0  81a67c96  Former state House Majority Leader Adam Hasner...   \n",
              "1  6e5ec6fb  Editor’s note: Subsequent to this article, VTD...   \n",
              "2  d9cd4895  The hypocritical Lib Dems want to ignore the r...   \n",
              "3  4a1a9b9f  Urgent action to boost the number of children ...   \n",
              "4  6d16fa40  By Ken Allen AFSCME Council 75 represents the ...   \n",
              "\n",
              "                                               title  our rating  \n",
              "0                              - The Washington Post         NaN  \n",
              "1                Rubio Comments on Iran Nuclear Deal         NaN  \n",
              "2  Climate Alarmists Caught Manipulating Temperat...         NaN  \n",
              "3  Who are the arsonists setting rural fires in W...         NaN  \n",
              "4  Diabetes prescriptions now cost NHS £1bn, figu...         NaN  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIqtzXljbZir"
      },
      "source": [
        "# merging title and the text into a single data form as input\n",
        "test['input'] = test['title']+' '+test['text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1WYww36bZis",
        "outputId": "e30f1a15-e399-4f8d-a149-ff339918e3b9"
      },
      "source": [
        "# stemming and removing stop words with the help of our predefined text_process() function\n",
        "corpus = text_process(test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7XYMQrbbZis"
      },
      "source": [
        "#converting our cleaned text_corpus into count_vectorize data with predefined count_vectorization() function\n",
        "test_data, cv = count_vectorization(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwPZfIMwbZis"
      },
      "source": [
        "# prediction of the rating(FALSE,TRUE,partially false,other) of Task3a_testing.csv dataset with our classifier.\n",
        "predicted_rating = classifier.predict(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuQME1PqbZis",
        "outputId": "33c6e7fa-4ec3-4bdf-f2fe-a7e20f9fede0"
      },
      "source": [
        "predicted_rating"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['TRUE', 'FALSE', 'FALSE', 'other', 'FALSE', 'TRUE', 'FALSE',\n",
              "       'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE',\n",
              "       'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE',\n",
              "       'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE',\n",
              "       'FALSE', 'TRUE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE',\n",
              "       'FALSE', 'FALSE', 'TRUE', 'FALSE', 'other', 'FALSE', 'FALSE',\n",
              "       'FALSE', 'FALSE', 'FALSE', 'TRUE', 'TRUE', 'FALSE', 'FALSE',\n",
              "       'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'TRUE', 'FALSE',\n",
              "       'TRUE', 'FALSE', 'TRUE', 'FALSE', 'other', 'FALSE', 'FALSE',\n",
              "       'FALSE', 'TRUE', 'FALSE', 'TRUE', 'FALSE', 'FALSE', 'FALSE',\n",
              "       'FALSE', 'FALSE', 'FALSE', 'FALSE', 'TRUE', 'FALSE', 'FALSE',\n",
              "       'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE',\n",
              "       'TRUE', 'other', 'FALSE', 'TRUE', 'FALSE', 'FALSE', 'FALSE',\n",
              "       'FALSE', 'TRUE', 'TRUE', 'FALSE', 'TRUE', 'other', 'FALSE',\n",
              "       'other', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE',\n",
              "       'FALSE', 'FALSE', 'FALSE', 'FALSE', 'TRUE', 'TRUE', 'FALSE',\n",
              "       'TRUE', 'TRUE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE',\n",
              "       'FALSE', 'FALSE', 'TRUE', 'FALSE', 'FALSE', 'TRUE', 'FALSE',\n",
              "       'FALSE', 'TRUE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE',\n",
              "       'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'other',\n",
              "       'FALSE', 'FALSE', 'FALSE', 'TRUE', 'FALSE', 'FALSE', 'FALSE',\n",
              "       'other', 'FALSE', 'other', 'other', 'FALSE', 'FALSE', 'FALSE',\n",
              "       'FALSE', 'other', 'TRUE', 'FALSE', 'FALSE', 'other', 'FALSE',\n",
              "       'FALSE', 'FALSE', 'FALSE', 'TRUE', 'FALSE', 'FALSE',\n",
              "       'partially false', 'TRUE', 'other', 'FALSE', 'other', 'TRUE',\n",
              "       'FALSE', 'FALSE', 'other', 'FALSE', 'TRUE', 'FALSE', 'FALSE',\n",
              "       'FALSE', 'other', 'FALSE', 'FALSE', 'TRUE', 'FALSE', 'TRUE',\n",
              "       'TRUE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'TRUE', 'FALSE',\n",
              "       'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE',\n",
              "       'TRUE', 'FALSE', 'FALSE', 'FALSE', 'other', 'FALSE', 'FALSE',\n",
              "       'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE',\n",
              "       'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE',\n",
              "       'FALSE', 'FALSE', 'FALSE', 'TRUE', 'FALSE', 'FALSE', 'TRUE',\n",
              "       'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE',\n",
              "       'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE',\n",
              "       'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE',\n",
              "       'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE',\n",
              "       'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE',\n",
              "       'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE',\n",
              "       'FALSE', 'other', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'TRUE',\n",
              "       'other', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'other', 'FALSE',\n",
              "       'FALSE', 'FALSE', 'FALSE', 'TRUE', 'FALSE', 'FALSE', 'FALSE',\n",
              "       'TRUE', 'FALSE', 'FALSE', 'TRUE', 'FALSE', 'FALSE', 'FALSE',\n",
              "       'FALSE', 'TRUE', 'FALSE', 'TRUE', 'FALSE', 'FALSE', 'FALSE',\n",
              "       'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE',\n",
              "       'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE',\n",
              "       'FALSE', 'FALSE', 'FALSE', 'FALSE', 'TRUE', 'FALSE', 'FALSE',\n",
              "       'FALSE', 'FALSE', 'FALSE', 'FALSE', 'other', 'FALSE', 'FALSE',\n",
              "       'TRUE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE',\n",
              "       'FALSE', 'TRUE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE',\n",
              "       'FALSE', 'FALSE', 'TRUE', 'FALSE', 'FALSE', 'FALSE', 'FALSE',\n",
              "       'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE', 'FALSE',\n",
              "       'FALSE', 'FALSE'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dyt7NoDkbZit"
      },
      "source": [
        "<h5>Exporting the result as tsv with 'public_id' and 'predicted_rating' column.</h5>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIlqzx8CbZit"
      },
      "source": [
        "task3a_df = pd.DataFrame()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhD2xek-bZit"
      },
      "source": [
        "task3a_df['public_id'] = test['public_id']\n",
        "task3a_df['predicted_rating'] = predicted_rating"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8FE8-hebZit",
        "outputId": "89b15a04-52bd-4b33-aed4-433aa6162a4d"
      },
      "source": [
        "task3a_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>public_id</th>\n",
              "      <th>predicted_rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>81a67c96</td>\n",
              "      <td>TRUE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6e5ec6fb</td>\n",
              "      <td>FALSE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>d9cd4895</td>\n",
              "      <td>FALSE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4a1a9b9f</td>\n",
              "      <td>other</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6d16fa40</td>\n",
              "      <td>FALSE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  public_id predicted_rating\n",
              "0  81a67c96             TRUE\n",
              "1  6e5ec6fb            FALSE\n",
              "2  d9cd4895            FALSE\n",
              "3  4a1a9b9f            other\n",
              "4  6d16fa40            FALSE"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7j7bvrlLbZiu"
      },
      "source": [
        "task3a_df.to_csv('result/task3a.tsv', sep='\\t', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dAeksLBbZiu"
      },
      "source": [
        "<h6> Saving our trained classifer model.</h6>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu_cXpgqbZiu"
      },
      "source": [
        "#import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_aoC0IDbZiu"
      },
      "source": [
        "\"\"\"# save model to disk\n",
        "filename = 'xgbc_model.sav'\n",
        "pickle.dump(classifier, open(filename, 'wb'))\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVxGzpXvbZiu",
        "outputId": "7e44646a-6fae-40ca-a4ca-773d2656cd6f"
      },
      "source": [
        "\"\"\"# load the model from disk\n",
        "loaded_model = pickle.load(open(filename, 'rb'))\n",
        "result = loaded_model.score(X_test, y_test)\n",
        "print(result)\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9227894334494181\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}